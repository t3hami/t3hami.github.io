<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Understand Autoscaling Applications In Kubernetes Before Next Peak Hours | Tehami's Blog</title><meta name=keywords content="Kubernnetes,Cloud Computing,Autoscaling,Metrics Server,Nginx"><meta name=description content="What is Autoscaling?
Automatic scaling, is a cloud computing strategy that dynamically modifies the amount of computational resources required for an application. Normally measured by the number of active servers based on the load on the farm. For example, the number of servers hosting a web application can automatically increase or decrease based on the number of active users on your site. Because such metrics can fluctuate substantially during the day, and servers are a limited resource that cost money to run even when inactive, there is often an incentive to run “just enough” servers to support the current demand while still being able to handle sudden and large surges in activity. Autoscaling is useful for such situations because it may reduce the number of active servers when activity is low, and it can also increase the number of active servers when activity is high."><meta name=author content="Muhammad Tehami"><link rel=canonical href=http://t3hami.github.io/tech/post_3/><link crossorigin=anonymous href=/assets/css/stylesheet.e1f5c4cae44599655f7ff95195ff89c8cb3adbda94f2b1581a434ab2b4d4e6cf.css integrity="sha256-4fXEyuRFmWVff/lRlf+JyMs629qU8rFYGkNKsrTU5s8=" rel="preload stylesheet" as=style><link rel=icon href=http://t3hami.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://t3hami.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://t3hami.github.io/favicon-32x32.png><link rel=apple-touch-icon href=http://t3hami.github.io/apple-touch-icon.png><link rel=mask-icon href=http://t3hami.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://t3hami.github.io/tech/post_3/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://t3hami.github.io/tech/post_3/"><meta property="og:site_name" content="Tehami's Blog"><meta property="og:title" content="Understand Autoscaling Applications In Kubernetes Before Next Peak Hours"><meta property="og:description" content="What is Autoscaling? Automatic scaling, is a cloud computing strategy that dynamically modifies the amount of computational resources required for an application. Normally measured by the number of active servers based on the load on the farm. For example, the number of servers hosting a web application can automatically increase or decrease based on the number of active users on your site. Because such metrics can fluctuate substantially during the day, and servers are a limited resource that cost money to run even when inactive, there is often an incentive to run “just enough” servers to support the current demand while still being able to handle sudden and large surges in activity. Autoscaling is useful for such situations because it may reduce the number of active servers when activity is low, and it can also increase the number of active servers when activity is high."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="tech"><meta property="article:published_time" content="2021-12-07T23:20:15+05:00"><meta property="article:modified_time" content="2021-12-07T23:20:15+05:00"><meta property="article:tag" content="Kubernnetes"><meta property="article:tag" content="Cloud Computing"><meta property="article:tag" content="Autoscaling"><meta property="article:tag" content="Metrics Server"><meta property="article:tag" content="Nginx"><meta name=twitter:card content="summary"><meta name=twitter:title content="Understand Autoscaling Applications In Kubernetes Before Next Peak Hours"><meta name=twitter:description content="What is Autoscaling?
Automatic scaling, is a cloud computing strategy that dynamically modifies the amount of computational resources required for an application. Normally measured by the number of active servers based on the load on the farm. For example, the number of servers hosting a web application can automatically increase or decrease based on the number of active users on your site. Because such metrics can fluctuate substantially during the day, and servers are a limited resource that cost money to run even when inactive, there is often an incentive to run “just enough” servers to support the current demand while still being able to handle sudden and large surges in activity. Autoscaling is useful for such situations because it may reduce the number of active servers when activity is low, and it can also increase the number of active servers when activity is high."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Tech","item":"http://t3hami.github.io/tech/"},{"@type":"ListItem","position":2,"name":"Understand Autoscaling Applications In Kubernetes Before Next Peak Hours","item":"http://t3hami.github.io/tech/post_3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Understand Autoscaling Applications In Kubernetes Before Next Peak Hours","name":"Understand Autoscaling Applications In Kubernetes Before Next Peak Hours","description":"What is Autoscaling? Automatic scaling, is a cloud computing strategy that dynamically modifies the amount of computational resources required for an application. Normally measured by the number of active servers based on the load on the farm. For example, the number of servers hosting a web application can automatically increase or decrease based on the number of active users on your site. Because such metrics can fluctuate substantially during the day, and servers are a limited resource that cost money to run even when inactive, there is often an incentive to run “just enough” servers to support the current demand while still being able to handle sudden and large surges in activity. Autoscaling is useful for such situations because it may reduce the number of active servers when activity is low, and it can also increase the number of active servers when activity is high.\n","keywords":["Kubernnetes","Cloud Computing","Autoscaling","Metrics Server","Nginx"],"articleBody":"What is Autoscaling? Automatic scaling, is a cloud computing strategy that dynamically modifies the amount of computational resources required for an application. Normally measured by the number of active servers based on the load on the farm. For example, the number of servers hosting a web application can automatically increase or decrease based on the number of active users on your site. Because such metrics can fluctuate substantially during the day, and servers are a limited resource that cost money to run even when inactive, there is often an incentive to run “just enough” servers to support the current demand while still being able to handle sudden and large surges in activity. Autoscaling is useful for such situations because it may reduce the number of active servers when activity is low, and it can also increase the number of active servers when activity is high.\nBefore moving ahead to deploy our application on Kubernetes and Autoscale it, there are a couple of terms we need to be familiar with.\nInstance: A single server or machine that is in itself is an independent unit or we can say microservice. Autoscaling Group: The set of instances that are subject to autoscaling, as well as all associated policies and state data. Size: The number of instances in the autoscaling group at the moment. Desired Size: At any given time, the number of instances that the autoscaling group should have. The autoscaling group will try to launch (provision and attach) new instances if the size is less than the intended size. The autoscaling group will try to eliminate (detach and terminate) instances if the size exceeds the specified size. Minimum Size: The number of instance from which the targeted capacity is not allowed to go below. Maximum Size: The number of instance from which the targeted capacity is not allowed to go beyond. Metric: A measurement connected with the autoscaling group for which a time series of data points is created on a regular basis (such as CPU use, memory consumption, and network usage). Metric thresholds can be used to create autoscaling policies. Autoscaling Policy: In response to metrics reaching particular thresholds, a policy that specifies a modification to the autoscaling group’s targeted capacity (or, in some cases, its minimum and maximum size). Cooldown periods can be connected with scaling policies, preventing future scaling actions from occurring shortly after one. Changes to intended capacity could be incremental (increasing or decreasing by a specified number) or give a new desired capacity value. Policies that enhance desired capacity are referred to as “scaling out” or “scaling up,” while policies that reduce desired capacity are referred to as “scaling in” or “scaling down”. Vertical Scaling: Vertical scaling preserves your current infrastructure while increasing processing power. All you have to do is run it on an existing number of machines but with improved specs. By scaling up, you can improve the capacity and throughput of a single system. . Horizontal Scaling: Horizontal scaling increases the number of machine instances without improving the existing specs. Scale out to distribute processing power and load balancing across multiple servers. We will mainly focus on Horizontal Scaling in this article.\nSetting Up Environment Minikube Minikube is a local Kubernetes that focuses on making Kubernetes easy to learn and develop locally. Kubernetes is only a single command away if you have Docker (or similarly compatible) container tooling or a Virtual Machine environment. Head to the following docs: https://minikube.sigs.k8s.io/docs/start/\nKubectl Kubernetes clusters can be managed through the kubectl command line tool. https://minikube.sigs.k8s.io/docs/handbook/kubectl/\nNow we have minikube and kubectl to control our cluster from cli let’s start our very first Kubernetes Cluster:\n$ minikube start Deploying Nginx Server on Kubernetes Let’s deploy an application on Kubernetes. For the demo purpose we are using the most simplest of the deployment; an Nginx server.\nCreate autoscale namespace.\n$ kubectl create ns autoscale Create nginx-deployment.yaml file with following content:\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: autoscale spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 resources: requests: cpu: 250m memory: 100Mi limits: cpu: 300m memory: 150Mi Create deployment\n$ kubectl apply -f nginx-deployment.yaml Now we have to expose our application using NodePort service. Create nginx-service.yaml file with following content:\napiVersion: v1 kind: Service metadata: name: nginx namespace: autoscale labels: app: nginx spec: type: NodePort ports: - port: 8080 nodePort: 31934 targetPort: 80 protocol: TCP name: http selector: app: nginx Create service\n$ kubectl apply -f nginx-service.yaml Verify the Nginx server started and can be accessed using configured Node Port\n$ curl http://`minikube ip`:31934 We have successfully deployed a Nginx server on our Kubernetes Cluster. Now the problem is if our application starts getting a lot of traffic it’s performance will start to decline as there is only one replica(instance) of our deployment(application) running in the cluster. We can manually update the number of replicas in our deployment before peak hours but then it will be unnecessary use of resources during the time there is very less traffic.\nAutoscaling with Kubernetes Metric Server For Kubernetes built-in autoscaling pipelines, Metrics Server offers a scalable and efficient source of container resource metrics.\nThe Metrics API in the Kubernetes apiserver collects resource metrics from Kubelet and makes them available to Horizontal Pod Autoscaler and Vertical Pod Autoscaler. kubectl top may also access the metrics API.\nInstalling Metric Server By default there is no metric server installed in Cluster created by Minikube. Install the metric server using the following command:\n$ minikube addons disable heapster $ minikube addons enable metrics-server Note: Use following Helm Chart to install Metric Server if you are not using Minikube: https://github.com/kubernetes-sigs/metrics-server/tree/master/charts/metrics-server\nValidate Metric Server is running\n$ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE coredns-74ff55c5b-x42f6 1/1 Running 0 1d etcd-minikube 1/1 Running 0 1d kube-apiserver-minikube 1/1 Running 0 1d kube-controller-manager-minikube 1/1 Running 0 1d kube-proxy-k87m2 1/1 Running 0 1d kube-scheduler-minikube 1/1 Running 0 1d metrics-server-7894db45f8-wxzqd 1/1 Running 0 3m storage-provisioner 1/1 Running 0 1d Create nginx-autoscale.yaml with following content\napiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: nginx-autoscale namespace: autoscale spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: nginx-deployment minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 Now create Horizontal Pod Autoscaler\n$ kubectl apply -f nginx-autoscale.yaml Check current metrics\n$ kubectl top pod -n autoscale NAME CPU(cores) MEMORY(bytes) nginx-deployment-67459f4f86-6hq6g 0m 2Mi There is only one pod running which is very obvious as we can see there’s no traffic currently coming to the Nginx server.\nCreating Load With Apache Bench The Apache Bench(ab) is a load testing and benchmarking tool for HTTP servers. It’s easy to use and may be started from the terminal. Install for your platform: https://httpd.apache.org/docs/2.4/programs/ab.html\nVerify that you have it working by checking Apache Bench version.\n$ ab -V Now open two terminals\n1st terminal to monitor pods and their resource usage\n$ watch kubectl top pod -n autoscale 2nd terminal to send load to Nginx using Apache Bench. Here we are sending a total of 200000 requests and 200 concurrent requests. This will generate enough load to increase CPU utilization above 50%.\nab -n 200000 -c 200 http://`minikube ip`:31934/ Soon you will see the surge in resource usage and pods getting autoscale. In my machine the number of pods increased to 4.\n$ NAME CPU(cores) MEMORY(bytes) nginx-deployment-67459f4f86-6hq6g 139m 2Mi nginx-deployment-67459f4f86-h6f5m 116m 2Mi nginx-deployment-67459f4f86-jt66v 52m 2Mi nginx-deployment-67459f4f86-x55gv 56m 2Mi Once all requests are completed the pods will downscale to 1 again.\nConclusion Kubernetes Horizontal Autoscaling takes care of up scaling and down scaling pods based on the resource usage metrics specified. It eliminates the need of manually changing the configuration to meet the current resource usage demand.\n","wordCount":"1287","inLanguage":"en","datePublished":"2021-12-07T23:20:15+05:00","dateModified":"2021-12-07T23:20:15+05:00","author":{"@type":"Person","name":"Muhammad Tehami"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://t3hami.github.io/tech/post_3/"},"publisher":{"@type":"Organization","name":"Tehami's Blog","logo":{"@type":"ImageObject","url":"http://t3hami.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://t3hami.github.io/ accesskey=h title="Tehami's Blog (Alt + H)">Tehami's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://t3hami.github.io/tech/ title=Tech><span>Tech</span></a></li><li><a href=http://t3hami.github.io/books/ title=Books><span>Books</span></a></li><li><a href=http://t3hami.github.io/travel/ title=Travel><span>Travel</span></a></li><li><a href=http://t3hami.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=http://t3hami.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://t3hami.github.io/about/ title=About><span>About</span></a></li><li><a href=http://t3hami.github.io/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://t3hami.github.io/>Home</a>&nbsp;»&nbsp;<a href=http://t3hami.github.io/tech/>Tech</a></div><h1 class="post-title entry-hint-parent">Understand Autoscaling Applications In Kubernetes Before Next Peak Hours</h1><div class=post-meta><span title='2021-12-07 23:20:15 +0500 +0500'>December 7, 2021</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Muhammad Tehami</div></header><div class=post-content><h2 id=what-is-autoscaling>What is Autoscaling?<a hidden class=anchor aria-hidden=true href=#what-is-autoscaling>#</a></h2><p>Automatic scaling, is a cloud computing strategy that dynamically modifies the amount of computational resources required for an application. Normally measured by the number of active servers based on the load on the farm. For example, the number of servers hosting a web application can automatically increase or decrease based on the number of active users on your site. Because such metrics can fluctuate substantially during the day, and servers are a limited resource that cost money to run even when inactive, there is often an incentive to run “just enough” servers to support the current demand while still being able to handle sudden and large surges in activity. Autoscaling is useful for such situations because it may reduce the number of active servers when activity is low, and it can also increase the number of active servers when activity is high.</p><p><img alt="How the Autoscaling System Works" loading=lazy src=/images/posts/post_3/how-the-autoscaling-system-works.gif></p><p>Before moving ahead to deploy our application on Kubernetes and Autoscale it, there are a couple of terms we need to be familiar with.</p><ul><li><strong>Instance:</strong> A single server or machine that is in itself is an independent unit or we can say microservice.
Autoscaling Group: The set of instances that are subject to autoscaling, as well as all associated policies and state data.</li><li><strong>Size:</strong> The number of instances in the autoscaling group at the moment.</li><li><strong>Desired Size:</strong> At any given time, the number of instances that the autoscaling group should have. The autoscaling group will try to launch (provision and attach) new instances if the size is less than the intended size. The autoscaling group will try to eliminate (detach and terminate) instances if the size exceeds the specified size.</li><li><strong>Minimum Size:</strong> The number of instance from which the targeted capacity is not allowed to go below.</li><li><strong>Maximum Size:</strong> The number of instance from which the targeted capacity is not allowed to go beyond.</li><li><strong>Metric:</strong> A measurement connected with the autoscaling group for which a time series of data points is created on a regular basis (such as CPU use, memory consumption, and network usage). Metric thresholds can be used to create autoscaling policies.</li><li><strong>Autoscaling Policy:</strong> In response to metrics reaching particular thresholds, a policy that specifies a modification to the autoscaling group’s targeted capacity (or, in some cases, its minimum and maximum size). Cooldown periods can be connected with scaling policies, preventing future scaling actions from occurring shortly after one. Changes to intended capacity could be incremental (increasing or decreasing by a specified number) or give a new desired capacity value. Policies that enhance desired capacity are referred to as “scaling out” or “scaling up,” while policies that reduce desired capacity are referred to as “scaling in” or “scaling down”.</li><li><strong>Vertical Scaling:</strong> Vertical scaling preserves your current infrastructure while increasing processing power. All you have to do is run it on an existing number of machines but with improved specs. By scaling up, you can improve the capacity and throughput of a single system. .</li><li><strong>Horizontal Scaling:</strong> Horizontal scaling increases the number of machine instances without improving the existing specs. Scale out to distribute processing power and load balancing across multiple servers.</li></ul><p>We will mainly focus on Horizontal Scaling in this article.</p><h2 id=setting-up-environment>Setting Up Environment<a hidden class=anchor aria-hidden=true href=#setting-up-environment>#</a></h2><h3 id=minikube>Minikube<a hidden class=anchor aria-hidden=true href=#minikube>#</a></h3><p><img alt="Minikube Logo" loading=lazy src=/images/posts/post_3/minikube.png></p><p>Minikube is a local Kubernetes that focuses on making Kubernetes easy to learn and develop locally. Kubernetes is only a single command away if you have Docker (or similarly compatible) container tooling or a Virtual Machine environment. Head to the following docs: <a href=https://minikube.sigs.k8s.io/docs/start/>https://minikube.sigs.k8s.io/docs/start/</a></p><h3 id=kubectl>Kubectl<a hidden class=anchor aria-hidden=true href=#kubectl>#</a></h3><p>Kubernetes clusters can be managed through the kubectl command line tool.
<a href=https://minikube.sigs.k8s.io/docs/handbook/kubectl/>https://minikube.sigs.k8s.io/docs/handbook/kubectl/</a></p><p>Now we have minikube and kubectl to control our cluster from cli let’s start our very first Kubernetes Cluster:</p><pre tabindex=0><code>$ minikube start
</code></pre><h2 id=deploying-nginx-server-on-kubernetes>Deploying Nginx Server on Kubernetes<a hidden class=anchor aria-hidden=true href=#deploying-nginx-server-on-kubernetes>#</a></h2><p>Let’s deploy an application on Kubernetes. For the demo purpose we are using the most simplest of the deployment; an Nginx server.</p><p>Create autoscale namespace.</p><pre tabindex=0><code>$ kubectl create ns autoscale
</code></pre><p>Create nginx-deployment.yaml file with following content:</p><pre tabindex=0><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: autoscale
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 250m
            memory: 100Mi
          limits:
            cpu: 300m
            memory: 150Mi
</code></pre><p>Create deployment</p><pre tabindex=0><code>$ kubectl apply -f nginx-deployment.yaml
</code></pre><p>Now we have to expose our application using NodePort service. Create nginx-service.yaml file with following content:</p><pre tabindex=0><code>apiVersion: v1
kind: Service
metadata:
  name: nginx
  namespace: autoscale
  labels:
    app: nginx
spec:
  type: NodePort
  ports:
  - port: 8080
    nodePort: 31934
    targetPort: 80
    protocol: TCP
    name: http
  selector:
    app: nginx
</code></pre><p>Create service</p><pre tabindex=0><code>$ kubectl apply -f nginx-service.yaml
</code></pre><p>Verify the Nginx server started and can be accessed using configured Node Port</p><pre tabindex=0><code>$ curl http://`minikube ip`:31934
</code></pre><p>We have successfully deployed a Nginx server on our Kubernetes Cluster. Now the problem is if our application starts getting a lot of traffic it’s performance will start to decline as there is only one replica(instance) of our deployment(application) running in the cluster. We can manually update the number of replicas in our deployment before peak hours but then it will be unnecessary use of resources during the time there is very less traffic.</p><h2 id=autoscaling-with-kubernetes-metric-server>Autoscaling with Kubernetes Metric Server<a hidden class=anchor aria-hidden=true href=#autoscaling-with-kubernetes-metric-server>#</a></h2><p>For Kubernetes built-in autoscaling pipelines, Metrics Server offers a scalable and efficient source of container resource metrics.</p><p>The Metrics API in the Kubernetes apiserver collects resource metrics from Kubelet and makes them available to Horizontal Pod Autoscaler and Vertical Pod Autoscaler. kubectl top may also access the metrics API.</p><h3 id=installing-metric-server>Installing Metric Server<a hidden class=anchor aria-hidden=true href=#installing-metric-server>#</a></h3><p>By default there is no metric server installed in Cluster created by Minikube. Install the metric server using the following command:</p><pre tabindex=0><code>$ minikube addons disable heapster
$ minikube addons enable metrics-server
</code></pre><p><em>Note</em>: Use following Helm Chart to install Metric Server if you are not using Minikube: <a href=https://github.com/kubernetes-sigs/metrics-server/tree/master/charts/metrics-server>https://github.com/kubernetes-sigs/metrics-server/tree/master/charts/metrics-server</a></p><p>Validate Metric Server is running</p><pre tabindex=0><code>$ kubectl get po -n kube-system
NAME                              READY   STATUS       RESTARTS  AGE
coredns-74ff55c5b-x42f6           1/1     Running       0         1d
etcd-minikube                     1/1     Running       0         1d
kube-apiserver-minikube           1/1     Running       0         1d
kube-controller-manager-minikube  1/1     Running       0         1d
kube-proxy-k87m2                  1/1     Running       0         1d
kube-scheduler-minikube           1/1     Running       0         1d
metrics-server-7894db45f8-wxzqd   1/1     Running       0         3m
storage-provisioner               1/1     Running       0         1d
</code></pre><p>Create nginx-autoscale.yaml with following content</p><pre tabindex=0><code>apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-autoscale
  namespace: autoscale
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
</code></pre><p>Now create Horizontal Pod Autoscaler</p><pre tabindex=0><code>$ kubectl apply -f nginx-autoscale.yaml
</code></pre><p>Check current metrics</p><pre tabindex=0><code>$ kubectl top pod -n autoscale
NAME                                CPU(cores)   MEMORY(bytes)
nginx-deployment-67459f4f86-6hq6g   0m           2Mi
</code></pre><p>There is only one pod running which is very obvious as we can see there’s no traffic currently coming to the Nginx server.</p><h3 id=creating-load-with-apache-bench>Creating Load With Apache Bench<a hidden class=anchor aria-hidden=true href=#creating-load-with-apache-bench>#</a></h3><p><img alt="Apache Bench Logo" loading=lazy src=/images/posts/post_3/apache-bench.png></p><p>The Apache Bench(ab) is a load testing and benchmarking tool for HTTP servers. It’s easy to use and may be started from the terminal. Install for your platform: <a href=https://httpd.apache.org/docs/2.4/programs/ab.html>https://httpd.apache.org/docs/2.4/programs/ab.html</a></p><p>Verify that you have it working by checking Apache Bench version.</p><pre tabindex=0><code>$ ab -V
</code></pre><p>Now open two terminals</p><p>1st terminal to monitor pods and their resource usage</p><pre tabindex=0><code>$ watch kubectl top pod -n autoscale
</code></pre><p>2nd terminal to send load to Nginx using Apache Bench. Here we are sending a total of 200000 requests and 200 concurrent requests. This will generate enough load to increase CPU utilization above 50%.</p><pre tabindex=0><code>ab -n 200000 -c 200 http://`minikube ip`:31934/
</code></pre><p>Soon you will see the surge in resource usage and pods getting autoscale. In my machine the number of pods increased to 4.</p><pre tabindex=0><code>$ NAME                                CPU(cores)   MEMORY(bytes)
nginx-deployment-67459f4f86-6hq6g   139m         2Mi
nginx-deployment-67459f4f86-h6f5m   116m         2Mi
nginx-deployment-67459f4f86-jt66v   52m          2Mi
nginx-deployment-67459f4f86-x55gv   56m          2Mi
</code></pre><p>Once all requests are completed the pods will downscale to 1 again.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Kubernetes Horizontal Autoscaling takes care of up scaling and down scaling pods based on the resource usage metrics specified. It eliminates the need of manually changing the configuration to meet the current resource usage demand.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://t3hami.github.io/tags/kubernnetes/>Kubernnetes</a></li><li><a href=http://t3hami.github.io/tags/cloud-computing/>Cloud Computing</a></li><li><a href=http://t3hami.github.io/tags/autoscaling/>Autoscaling</a></li><li><a href=http://t3hami.github.io/tags/metrics-server/>Metrics Server</a></li><li><a href=http://t3hami.github.io/tags/nginx/>Nginx</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://t3hami.github.io/>Tehami's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>