<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Understand Autoscaling Applications In Kubernetes Before Next Peak Hours | Tehami's Blog</title><meta name=keywords content="Kubernnetes,Cloud Computing,Autoscaling,Metrics Server,Nginx"><meta name=description content="What is Autoscaling?
Automatic scaling, is a cloud computing strategy that dynamically modifies the amount of computational resources required for an application. Normally measured by the number of active servers based on the load on the farm. For example, the number of servers hosting a web application can automatically increase or decrease based on the number of active users on your site. Because such metrics can fluctuate substantially during the day, and servers are a limited resource that cost money to run even when inactive, there is often an incentive to run “just enough” servers to support the current demand while still being able to handle sudden and large surges in activity. Autoscaling is useful for such situations because it may reduce the number of active servers when activity is low, and it can also increase the number of active servers when activity is high."><meta name=author content="Muhammad Tehami"><link rel=canonical href=http://t3hami.github.io/tech/post_3/><link crossorigin=anonymous href=/assets/css/stylesheet.e1f5c4cae44599655f7ff95195ff89c8cb3adbda94f2b1581a434ab2b4d4e6cf.css integrity="sha256-4fXEyuRFmWVff/lRlf+JyMs629qU8rFYGkNKsrTU5s8=" rel="preload stylesheet" as=style><link rel=icon href=http://t3hami.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://t3hami.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://t3hami.github.io/favicon-32x32.png><link rel=apple-touch-icon href=http://t3hami.github.io/apple-touch-icon.png><link rel=mask-icon href=http://t3hami.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://t3hami.github.io/tech/post_3/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://t3hami.github.io/tech/post_3/"><meta property="og:site_name" content="Tehami's Blog"><meta property="og:title" content="Understand Autoscaling Applications In Kubernetes Before Next Peak Hours"><meta property="og:description" content="What is Autoscaling? Automatic scaling, is a cloud computing strategy that dynamically modifies the amount of computational resources required for an application. Normally measured by the number of active servers based on the load on the farm. For example, the number of servers hosting a web application can automatically increase or decrease based on the number of active users on your site. Because such metrics can fluctuate substantially during the day, and servers are a limited resource that cost money to run even when inactive, there is often an incentive to run “just enough” servers to support the current demand while still being able to handle sudden and large surges in activity. Autoscaling is useful for such situations because it may reduce the number of active servers when activity is low, and it can also increase the number of active servers when activity is high."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="tech"><meta property="article:published_time" content="2021-12-07T23:20:15+05:00"><meta property="article:modified_time" content="2021-12-07T23:20:15+05:00"><meta property="article:tag" content="Kubernnetes"><meta property="article:tag" content="Cloud Computing"><meta property="article:tag" content="Autoscaling"><meta property="article:tag" content="Metrics Server"><meta property="article:tag" content="Nginx"><meta property="og:image" content="http://t3hami.github.io/images/avatar.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://t3hami.github.io/images/avatar.jpg"><meta name=twitter:title content="Understand Autoscaling Applications In Kubernetes Before Next Peak Hours"><meta name=twitter:description content="What is Autoscaling?
Automatic scaling, is a cloud computing strategy that dynamically modifies the amount of computational resources required for an application. Normally measured by the number of active servers based on the load on the farm. For example, the number of servers hosting a web application can automatically increase or decrease based on the number of active users on your site. Because such metrics can fluctuate substantially during the day, and servers are a limited resource that cost money to run even when inactive, there is often an incentive to run “just enough” servers to support the current demand while still being able to handle sudden and large surges in activity. Autoscaling is useful for such situations because it may reduce the number of active servers when activity is low, and it can also increase the number of active servers when activity is high."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Tech","item":"http://t3hami.github.io/tech/"},{"@type":"ListItem","position":2,"name":"Understand Autoscaling Applications In Kubernetes Before Next Peak Hours","item":"http://t3hami.github.io/tech/post_3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Understand Autoscaling Applications In Kubernetes Before Next Peak Hours","name":"Understand Autoscaling Applications In Kubernetes Before Next Peak Hours","description":"What is Autoscaling? Automatic scaling, is a cloud computing strategy that dynamically modifies the amount of computational resources required for an application. Normally measured by the number of active servers based on the load on the farm. For example, the number of servers hosting a web application can automatically increase or decrease based on the number of active users on your site. Because such metrics can fluctuate substantially during the day, and servers are a limited resource that cost money to run even when inactive, there is often an incentive to run “just enough” servers to support the current demand while still being able to handle sudden and large surges in activity. Autoscaling is useful for such situations because it may reduce the number of active servers when activity is low, and it can also increase the number of active servers when activity is high.\n","keywords":["Kubernnetes","Cloud Computing","Autoscaling","Metrics Server","Nginx"],"articleBody":"What is Autoscaling? Automatic scaling, is a cloud computing strategy that dynamically modifies the amount of computational resources required for an application. Normally measured by the number of active servers based on the load on the farm. For example, the number of servers hosting a web application can automatically increase or decrease based on the number of active users on your site. Because such metrics can fluctuate substantially during the day, and servers are a limited resource that cost money to run even when inactive, there is often an incentive to run “just enough” servers to support the current demand while still being able to handle sudden and large surges in activity. Autoscaling is useful for such situations because it may reduce the number of active servers when activity is low, and it can also increase the number of active servers when activity is high.\nBefore moving ahead to deploy our application on Kubernetes and Autoscale it, there are a couple of terms we need to be familiar with.\nInstance: A single server or machine that is in itself is an independent unit or we can say microservice. Autoscaling Group: The set of instances that are subject to autoscaling, as well as all associated policies and state data. Size: The number of instances in the autoscaling group at the moment. Desired Size: At any given time, the number of instances that the autoscaling group should have. The autoscaling group will try to launch (provision and attach) new instances if the size is less than the intended size. The autoscaling group will try to eliminate (detach and terminate) instances if the size exceeds the specified size. Minimum Size: The number of instance from which the targeted capacity is not allowed to go below. Maximum Size: The number of instance from which the targeted capacity is not allowed to go beyond. Metric: A measurement connected with the autoscaling group for which a time series of data points is created on a regular basis (such as CPU use, memory consumption, and network usage). Metric thresholds can be used to create autoscaling policies. Autoscaling Policy: In response to metrics reaching particular thresholds, a policy that specifies a modification to the autoscaling group’s targeted capacity (or, in some cases, its minimum and maximum size). Cooldown periods can be connected with scaling policies, preventing future scaling actions from occurring shortly after one. Changes to intended capacity could be incremental (increasing or decreasing by a specified number) or give a new desired capacity value. Policies that enhance desired capacity are referred to as “scaling out” or “scaling up,” while policies that reduce desired capacity are referred to as “scaling in” or “scaling down”. Vertical Scaling: Vertical scaling preserves your current infrastructure while increasing processing power. All you have to do is run it on an existing number of machines but with improved specs. By scaling up, you can improve the capacity and throughput of a single system. . Horizontal Scaling: Horizontal scaling increases the number of machine instances without improving the existing specs. Scale out to distribute processing power and load balancing across multiple servers. We will mainly focus on Horizontal Scaling in this article.\nSetting Up Environment Minikube Minikube is a local Kubernetes that focuses on making Kubernetes easy to learn and develop locally. Kubernetes is only a single command away if you have Docker (or similarly compatible) container tooling or a Virtual Machine environment. Head to the following docs: https://minikube.sigs.k8s.io/docs/start/\nKubectl Kubernetes clusters can be managed through the kubectl command line tool. https://minikube.sigs.k8s.io/docs/handbook/kubectl/\nNow we have minikube and kubectl to control our cluster from cli let’s start our very first Kubernetes Cluster:\n$ minikube start Deploying Nginx Server on Kubernetes Let’s deploy an application on Kubernetes. For the demo purpose we are using the most simplest of the deployment; an Nginx server.\nCreate autoscale namespace.\n$ kubectl create ns autoscale Create nginx-deployment.yaml file with following content:\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: autoscale spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 resources: requests: cpu: 250m memory: 100Mi limits: cpu: 300m memory: 150Mi Create deployment\n$ kubectl apply -f nginx-deployment.yaml Now we have to expose our application using NodePort service. Create nginx-service.yaml file with following content:\napiVersion: v1 kind: Service metadata: name: nginx namespace: autoscale labels: app: nginx spec: type: NodePort ports: - port: 8080 nodePort: 31934 targetPort: 80 protocol: TCP name: http selector: app: nginx Create service\n$ kubectl apply -f nginx-service.yaml Verify the Nginx server started and can be accessed using configured Node Port\n$ curl http://`minikube ip`:31934 We have successfully deployed a Nginx server on our Kubernetes Cluster. Now the problem is if our application starts getting a lot of traffic it’s performance will start to decline as there is only one replica(instance) of our deployment(application) running in the cluster. We can manually update the number of replicas in our deployment before peak hours but then it will be unnecessary use of resources during the time there is very less traffic.\nAutoscaling with Kubernetes Metric Server For Kubernetes built-in autoscaling pipelines, Metrics Server offers a scalable and efficient source of container resource metrics.\nThe Metrics API in the Kubernetes apiserver collects resource metrics from Kubelet and makes them available to Horizontal Pod Autoscaler and Vertical Pod Autoscaler. kubectl top may also access the metrics API.\nInstalling Metric Server By default there is no metric server installed in Cluster created by Minikube. Install the metric server using the following command:\n$ minikube addons disable heapster $ minikube addons enable metrics-server Note: Use following Helm Chart to install Metric Server if you are not using Minikube: https://github.com/kubernetes-sigs/metrics-server/tree/master/charts/metrics-server\nValidate Metric Server is running\n$ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE coredns-74ff55c5b-x42f6 1/1 Running 0 1d etcd-minikube 1/1 Running 0 1d kube-apiserver-minikube 1/1 Running 0 1d kube-controller-manager-minikube 1/1 Running 0 1d kube-proxy-k87m2 1/1 Running 0 1d kube-scheduler-minikube 1/1 Running 0 1d metrics-server-7894db45f8-wxzqd 1/1 Running 0 3m storage-provisioner 1/1 Running 0 1d Create nginx-autoscale.yaml with following content\napiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: nginx-autoscale namespace: autoscale spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: nginx-deployment minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 Now create Horizontal Pod Autoscaler\n$ kubectl apply -f nginx-autoscale.yaml Check current metrics\n$ kubectl top pod -n autoscale NAME CPU(cores) MEMORY(bytes) nginx-deployment-67459f4f86-6hq6g 0m 2Mi There is only one pod running which is very obvious as we can see there’s no traffic currently coming to the Nginx server.\nCreating Load With Apache Bench The Apache Bench(ab) is a load testing and benchmarking tool for HTTP servers. It’s easy to use and may be started from the terminal. Install for your platform: https://httpd.apache.org/docs/2.4/programs/ab.html\nVerify that you have it working by checking Apache Bench version.\n$ ab -V Now open two terminals\n1st terminal to monitor pods and their resource usage\n$ watch kubectl top pod -n autoscale 2nd terminal to send load to Nginx using Apache Bench. Here we are sending a total of 200000 requests and 200 concurrent requests. This will generate enough load to increase CPU utilization above 50%.\nab -n 200000 -c 200 http://`minikube ip`:31934/ Soon you will see the surge in resource usage and pods getting autoscale. In my machine the number of pods increased to 4.\n$ NAME CPU(cores) MEMORY(bytes) nginx-deployment-67459f4f86-6hq6g 139m 2Mi nginx-deployment-67459f4f86-h6f5m 116m 2Mi nginx-deployment-67459f4f86-jt66v 52m 2Mi nginx-deployment-67459f4f86-x55gv 56m 2Mi Once all requests are completed the pods will downscale to 1 again.\nConclusion Kubernetes Horizontal Autoscaling takes care of up scaling and down scaling pods based on the resource usage metrics specified. It eliminates the need of manually changing the configuration to meet the current resource usage demand.\n","wordCount":"1287","inLanguage":"en","image":"http://t3hami.github.io/images/avatar.jpg","datePublished":"2021-12-07T23:20:15+05:00","dateModified":"2021-12-07T23:20:15+05:00","author":{"@type":"Person","name":"Muhammad Tehami"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://t3hami.github.io/tech/post_3/"},"publisher":{"@type":"Organization","name":"Tehami's Blog","logo":{"@type":"ImageObject","url":"http://t3hami.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://t3hami.github.io/ accesskey=h title="Tehami's Blog (Alt + H)">Tehami's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://t3hami.github.io/tech/ title=Tech><span>Tech</span></a></li><li><a href=http://t3hami.github.io/books/ title=Books><span>Books</span></a></li><li><a href=http://t3hami.github.io/travel/ title=Travel><span>Travel</span></a></li><li><a href=http://t3hami.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=http://t3hami.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://t3hami.github.io/about/ title=About><span>About</span></a></li><li><a href=http://t3hami.github.io/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://t3hami.github.io/>Home</a>&nbsp;»&nbsp;<a href=http://t3hami.github.io/tech/>Tech</a></div><h1 class="post-title entry-hint-parent">Understand Autoscaling Applications In Kubernetes Before Next Peak Hours</h1><div class=post-meta><span title='2021-12-07 23:20:15 +0500 +0500'>December 7, 2021</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Muhammad Tehami</div></header><div class=post-content><h2 id=what-is-autoscaling>What is Autoscaling?<a hidden class=anchor aria-hidden=true href=#what-is-autoscaling>#</a></h2><p>Automatic scaling, is a cloud computing strategy that dynamically modifies the amount of computational resources required for an application. Normally measured by the number of active servers based on the load on the farm. For example, the number of servers hosting a web application can automatically increase or decrease based on the number of active users on your site. Because such metrics can fluctuate substantially during the day, and servers are a limited resource that cost money to run even when inactive, there is often an incentive to run “just enough” servers to support the current demand while still being able to handle sudden and large surges in activity. Autoscaling is useful for such situations because it may reduce the number of active servers when activity is low, and it can also increase the number of active servers when activity is high.</p><p><img alt="How the Autoscaling System Works" loading=lazy src=/images/posts/post_3/how-the-autoscaling-system-works.gif></p><p>Before moving ahead to deploy our application on Kubernetes and Autoscale it, there are a couple of terms we need to be familiar with.</p><ul><li><strong>Instance:</strong> A single server or machine that is in itself is an independent unit or we can say microservice.
Autoscaling Group: The set of instances that are subject to autoscaling, as well as all associated policies and state data.</li><li><strong>Size:</strong> The number of instances in the autoscaling group at the moment.</li><li><strong>Desired Size:</strong> At any given time, the number of instances that the autoscaling group should have. The autoscaling group will try to launch (provision and attach) new instances if the size is less than the intended size. The autoscaling group will try to eliminate (detach and terminate) instances if the size exceeds the specified size.</li><li><strong>Minimum Size:</strong> The number of instance from which the targeted capacity is not allowed to go below.</li><li><strong>Maximum Size:</strong> The number of instance from which the targeted capacity is not allowed to go beyond.</li><li><strong>Metric:</strong> A measurement connected with the autoscaling group for which a time series of data points is created on a regular basis (such as CPU use, memory consumption, and network usage). Metric thresholds can be used to create autoscaling policies.</li><li><strong>Autoscaling Policy:</strong> In response to metrics reaching particular thresholds, a policy that specifies a modification to the autoscaling group’s targeted capacity (or, in some cases, its minimum and maximum size). Cooldown periods can be connected with scaling policies, preventing future scaling actions from occurring shortly after one. Changes to intended capacity could be incremental (increasing or decreasing by a specified number) or give a new desired capacity value. Policies that enhance desired capacity are referred to as “scaling out” or “scaling up,” while policies that reduce desired capacity are referred to as “scaling in” or “scaling down”.</li><li><strong>Vertical Scaling:</strong> Vertical scaling preserves your current infrastructure while increasing processing power. All you have to do is run it on an existing number of machines but with improved specs. By scaling up, you can improve the capacity and throughput of a single system. .</li><li><strong>Horizontal Scaling:</strong> Horizontal scaling increases the number of machine instances without improving the existing specs. Scale out to distribute processing power and load balancing across multiple servers.</li></ul><p>We will mainly focus on Horizontal Scaling in this article.</p><h2 id=setting-up-environment>Setting Up Environment<a hidden class=anchor aria-hidden=true href=#setting-up-environment>#</a></h2><h3 id=minikube>Minikube<a hidden class=anchor aria-hidden=true href=#minikube>#</a></h3><p><img alt="Minikube Logo" loading=lazy src=/images/posts/post_3/minikube.png></p><p>Minikube is a local Kubernetes that focuses on making Kubernetes easy to learn and develop locally. Kubernetes is only a single command away if you have Docker (or similarly compatible) container tooling or a Virtual Machine environment. Head to the following docs: <a href=https://minikube.sigs.k8s.io/docs/start/>https://minikube.sigs.k8s.io/docs/start/</a></p><h3 id=kubectl>Kubectl<a hidden class=anchor aria-hidden=true href=#kubectl>#</a></h3><p>Kubernetes clusters can be managed through the kubectl command line tool.
<a href=https://minikube.sigs.k8s.io/docs/handbook/kubectl/>https://minikube.sigs.k8s.io/docs/handbook/kubectl/</a></p><p>Now we have minikube and kubectl to control our cluster from cli let’s start our very first Kubernetes Cluster:</p><pre tabindex=0><code>$ minikube start
</code></pre><h2 id=deploying-nginx-server-on-kubernetes>Deploying Nginx Server on Kubernetes<a hidden class=anchor aria-hidden=true href=#deploying-nginx-server-on-kubernetes>#</a></h2><p>Let’s deploy an application on Kubernetes. For the demo purpose we are using the most simplest of the deployment; an Nginx server.</p><p>Create autoscale namespace.</p><pre tabindex=0><code>$ kubectl create ns autoscale
</code></pre><p>Create nginx-deployment.yaml file with following content:</p><pre tabindex=0><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: autoscale
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 250m
            memory: 100Mi
          limits:
            cpu: 300m
            memory: 150Mi
</code></pre><p>Create deployment</p><pre tabindex=0><code>$ kubectl apply -f nginx-deployment.yaml
</code></pre><p>Now we have to expose our application using NodePort service. Create nginx-service.yaml file with following content:</p><pre tabindex=0><code>apiVersion: v1
kind: Service
metadata:
  name: nginx
  namespace: autoscale
  labels:
    app: nginx
spec:
  type: NodePort
  ports:
  - port: 8080
    nodePort: 31934
    targetPort: 80
    protocol: TCP
    name: http
  selector:
    app: nginx
</code></pre><p>Create service</p><pre tabindex=0><code>$ kubectl apply -f nginx-service.yaml
</code></pre><p>Verify the Nginx server started and can be accessed using configured Node Port</p><pre tabindex=0><code>$ curl http://`minikube ip`:31934
</code></pre><p>We have successfully deployed a Nginx server on our Kubernetes Cluster. Now the problem is if our application starts getting a lot of traffic it’s performance will start to decline as there is only one replica(instance) of our deployment(application) running in the cluster. We can manually update the number of replicas in our deployment before peak hours but then it will be unnecessary use of resources during the time there is very less traffic.</p><h2 id=autoscaling-with-kubernetes-metric-server>Autoscaling with Kubernetes Metric Server<a hidden class=anchor aria-hidden=true href=#autoscaling-with-kubernetes-metric-server>#</a></h2><p>For Kubernetes built-in autoscaling pipelines, Metrics Server offers a scalable and efficient source of container resource metrics.</p><p>The Metrics API in the Kubernetes apiserver collects resource metrics from Kubelet and makes them available to Horizontal Pod Autoscaler and Vertical Pod Autoscaler. kubectl top may also access the metrics API.</p><h3 id=installing-metric-server>Installing Metric Server<a hidden class=anchor aria-hidden=true href=#installing-metric-server>#</a></h3><p>By default there is no metric server installed in Cluster created by Minikube. Install the metric server using the following command:</p><pre tabindex=0><code>$ minikube addons disable heapster
$ minikube addons enable metrics-server
</code></pre><p><em>Note</em>: Use following Helm Chart to install Metric Server if you are not using Minikube: <a href=https://github.com/kubernetes-sigs/metrics-server/tree/master/charts/metrics-server>https://github.com/kubernetes-sigs/metrics-server/tree/master/charts/metrics-server</a></p><p>Validate Metric Server is running</p><pre tabindex=0><code>$ kubectl get po -n kube-system
NAME                              READY   STATUS       RESTARTS  AGE
coredns-74ff55c5b-x42f6           1/1     Running       0         1d
etcd-minikube                     1/1     Running       0         1d
kube-apiserver-minikube           1/1     Running       0         1d
kube-controller-manager-minikube  1/1     Running       0         1d
kube-proxy-k87m2                  1/1     Running       0         1d
kube-scheduler-minikube           1/1     Running       0         1d
metrics-server-7894db45f8-wxzqd   1/1     Running       0         3m
storage-provisioner               1/1     Running       0         1d
</code></pre><p>Create nginx-autoscale.yaml with following content</p><pre tabindex=0><code>apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-autoscale
  namespace: autoscale
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
</code></pre><p>Now create Horizontal Pod Autoscaler</p><pre tabindex=0><code>$ kubectl apply -f nginx-autoscale.yaml
</code></pre><p>Check current metrics</p><pre tabindex=0><code>$ kubectl top pod -n autoscale
NAME                                CPU(cores)   MEMORY(bytes)
nginx-deployment-67459f4f86-6hq6g   0m           2Mi
</code></pre><p>There is only one pod running which is very obvious as we can see there’s no traffic currently coming to the Nginx server.</p><h3 id=creating-load-with-apache-bench>Creating Load With Apache Bench<a hidden class=anchor aria-hidden=true href=#creating-load-with-apache-bench>#</a></h3><p><img alt="Apache Bench Logo" loading=lazy src=/images/posts/post_3/apache-bench.png></p><p>The Apache Bench(ab) is a load testing and benchmarking tool for HTTP servers. It’s easy to use and may be started from the terminal. Install for your platform: <a href=https://httpd.apache.org/docs/2.4/programs/ab.html>https://httpd.apache.org/docs/2.4/programs/ab.html</a></p><p>Verify that you have it working by checking Apache Bench version.</p><pre tabindex=0><code>$ ab -V
</code></pre><p>Now open two terminals</p><p>1st terminal to monitor pods and their resource usage</p><pre tabindex=0><code>$ watch kubectl top pod -n autoscale
</code></pre><p>2nd terminal to send load to Nginx using Apache Bench. Here we are sending a total of 200000 requests and 200 concurrent requests. This will generate enough load to increase CPU utilization above 50%.</p><pre tabindex=0><code>ab -n 200000 -c 200 http://`minikube ip`:31934/
</code></pre><p>Soon you will see the surge in resource usage and pods getting autoscale. In my machine the number of pods increased to 4.</p><pre tabindex=0><code>$ NAME                                CPU(cores)   MEMORY(bytes)
nginx-deployment-67459f4f86-6hq6g   139m         2Mi
nginx-deployment-67459f4f86-h6f5m   116m         2Mi
nginx-deployment-67459f4f86-jt66v   52m          2Mi
nginx-deployment-67459f4f86-x55gv   56m          2Mi
</code></pre><p>Once all requests are completed the pods will downscale to 1 again.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Kubernetes Horizontal Autoscaling takes care of up scaling and down scaling pods based on the resource usage metrics specified. It eliminates the need of manually changing the configuration to meet the current resource usage demand.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://t3hami.github.io/tags/kubernnetes/>Kubernnetes</a></li><li><a href=http://t3hami.github.io/tags/cloud-computing/>Cloud Computing</a></li><li><a href=http://t3hami.github.io/tags/autoscaling/>Autoscaling</a></li><li><a href=http://t3hami.github.io/tags/metrics-server/>Metrics Server</a></li><li><a href=http://t3hami.github.io/tags/nginx/>Nginx</a></li></ul><nav class=paginav><a class=prev href=http://t3hami.github.io/tech/post_4/><span class=title>« Prev</span><br><span>Jenkins On-Demand Agents</span>
</a><a class=next href=http://t3hami.github.io/tech/post_2/><span class=title>Next »</span><br><span>Signing Git Commits with GPG</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Understand Autoscaling Applications In Kubernetes Before Next Peak Hours on x" href="https://x.com/intent/tweet/?text=Understand%20Autoscaling%20Applications%20In%20Kubernetes%20Before%20Next%20Peak%20Hours&amp;url=http%3a%2f%2ft3hami.github.io%2ftech%2fpost_3%2f&amp;hashtags=Kubernnetes%2cCloudComputing%2cAutoscaling%2cMetricsServer%2cNginx"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Understand Autoscaling Applications In Kubernetes Before Next Peak Hours on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2ft3hami.github.io%2ftech%2fpost_3%2f&amp;title=Understand%20Autoscaling%20Applications%20In%20Kubernetes%20Before%20Next%20Peak%20Hours&amp;summary=Understand%20Autoscaling%20Applications%20In%20Kubernetes%20Before%20Next%20Peak%20Hours&amp;source=http%3a%2f%2ft3hami.github.io%2ftech%2fpost_3%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Understand Autoscaling Applications In Kubernetes Before Next Peak Hours on reddit" href="https://reddit.com/submit?url=http%3a%2f%2ft3hami.github.io%2ftech%2fpost_3%2f&title=Understand%20Autoscaling%20Applications%20In%20Kubernetes%20Before%20Next%20Peak%20Hours"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Understand Autoscaling Applications In Kubernetes Before Next Peak Hours on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2ft3hami.github.io%2ftech%2fpost_3%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Understand Autoscaling Applications In Kubernetes Before Next Peak Hours on whatsapp" href="https://api.whatsapp.com/send?text=Understand%20Autoscaling%20Applications%20In%20Kubernetes%20Before%20Next%20Peak%20Hours%20-%20http%3a%2f%2ft3hami.github.io%2ftech%2fpost_3%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Understand Autoscaling Applications In Kubernetes Before Next Peak Hours on telegram" href="https://telegram.me/share/url?text=Understand%20Autoscaling%20Applications%20In%20Kubernetes%20Before%20Next%20Peak%20Hours&amp;url=http%3a%2f%2ft3hami.github.io%2ftech%2fpost_3%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Understand Autoscaling Applications In Kubernetes Before Next Peak Hours on ycombinator" href="https://news.ycombinator.com/submitlink?t=Understand%20Autoscaling%20Applications%20In%20Kubernetes%20Before%20Next%20Peak%20Hours&u=http%3a%2f%2ft3hami.github.io%2ftech%2fpost_3%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://t3hami.github.io/>Tehami's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>